\begin{abstract}
\section*{Abstract}
The amount of data generated by all the edge devices such as smartphones, wireless sensors, and IoT devices in the future can cause congestion at the cloud for further processing and thus increase response time for application requests. As a solution to this problem, emerging  Mobile Edge Computing architecture plays a major role as a complement of existing cloud computing. MEC improves the response time by moving network traffic and computational resources to the edge of the network and closer to the customer. MEC does not send the data to the cloud for further processing instead the edge network itself will analyze and process the data. It also has it's own limitations like the cost for infrastructure implementation and for prolongation and also movement at the edge devices can cause severe congestion and bring down the MEC servers. The burst request and everchanging environment at MEC face an issue in allocating computing and network resources. At this stage, Deep Reinforcement Learning based Resource Allocation scheme adapts to a mutative nature and allocates computing and network resources. Thus reduces the service time to users and fairness in the use of resources at MEC. DRLRA achieved better results in varying MEC environments than the traditional OSPF algorithm. 
\end{abstract}

% the actual content, usually separated over a number of sections
% each section is assigned a label, in order to be able to put a
% crossreference to it

\setlength{\parindent}{1em}
\setlength{\parskip}{0.1em}
\section{Introduction}
\label{sec:introduction}
\begin{itemize}
    \item Problem Definition - Upgrading the functions of mobile devices demands high computing resources, which does not fit at the mobile devices. These devices end up by getting the help from cloud i.e computing resources. Increasing in devices cause high traffic at cloud and cause network congestion and delay in fulfilling user requests.\cite{Sun2016:_MECIoT} \cite{Jia2019:_MEC}\cite{Wen2017:_IV}\cite{Kuljeet2018:_IIoT}
    \item Emerging technology Mobile Edge Computing MEC.
    \item Limitations of MEC
    \item Deep reinforcement learning resource allocation algorithm.\cite{Wiki:_RL}
    \item Merging Software Defined Network SDN \cite{soft:_SDN} with MEC.
\end{itemize}

\section{SDN Enabled MEC architecture}
\label{sec:SDN Enabled MEC architecture}
\begin{itemize}
    \item Implementing massive number of application at a single MEC is cost effective, hence limited number of applications deployed at single MEC. Consider the scenario, When the MEC in the vicinity of mobile does not have requested application, the request should be routed to corresponding MEC. That's where SDN plays important role by guiding the routing path.
    \item Three planes of SDN
    \begin{enumerate}
        \item Data Plane - 
        \item Control plane - Where DRL unit with DRLRA algorithm is implemented. and makes a sequence of decisions on allocating routing and computing resources.
        \item Application Plane
        
    \end{enumerate}
\end{itemize}

\section{Problem Definition}
\label{sec:Problem Definition}
\begin{itemize}
\item {Edge network routing delay} - The delay in routing the request to destined MECS.
\item {Data Processing delay } - When request reache the destined MECS, delay in processing the request.
\item{Variance of network resource allocation} - Allocating network routing resource should be stabilized, because several requests will be aggregated from MECS of certain regions which are destined to MECS hosting the required application.
\item{Variance of computing resource allocation} - Requests take different paths to reach destined MECS. Therefore, the computational load at MECS varies.
\end{itemize}

\section{Deep Reinforcement Learning Framework}
\label{sec:Deep Reinforcement Learning Framework:}
\begin{itemize}
    \item Episode - Sequence of states, actions, and rewards that occurs between initial state and termination state
    \item Policy Pi - is mapping from states of the environment to actions to be taken in those states \cite{Policy:_RL}
    \item Reward - is positive feedback given, when an agent performs good action.
    \item Action value function - value function Q(st, at) is to quantify the quality of the action made at state st.  \cite {Qrash:_DQN} 
\end{itemize}


\section{ DEEP REINFORCEMENT LEARNING BASED RESOURCE ALLOCATION ALGORITHM}
\label{sec:  DEEP REINFORCEMENT LEARNING BASED RESOURCE ALLOCATION ALGORITHM}
\begin{itemize}
    \item {Explaining training of an agent} 
    \item {Key elements of RL in DRLRA}
    \begin{itemize}
        \item {State} - aggregated requests for application at certain districts
        \item {Action} - The selection of the adjacent MEC by each of MECs to route the request to MECS hosting the required application.
        \item{Reward} - After performing an action agent gets a reward, whose goal is to minimize the average service time by balancing the allocation of computing and network resources.
    \end{itemize}
\end{itemize}


\section{PERFORMANCE EVALUATION}
\label{sec:PERFORMANCE EVALUATION}
\begin{itemize}
    \item {Average service time minimization} -Graph to be included to show the comparison between performance of traditional OSPF algorithm and smart DRLRA algorithm. Which of the algorithm performs better when aggregated requests at MECS increase and when number of deployed application at MEC increases
    \item{Resource Allocation Balancing Analysis} - To analyse which of the algorithm maintains stability of the network and computing resources by allocating them intelligently 
\end{itemize}

\section{Conclusion}
\label{sec:Conclusion}
This paper included the serious problem of assigning computing and network resources when the request burst in high volume by reducing the average service time. The DRL based DRLRA algorithm's capacity to understand the mutative environment and make a sequence of decisions makes itself fit for the MEC environment. Experiments conducted to evaluate the efficiency of DRLRA and OSPF algorithms, the output has proven that DRLRA is more effective in the varying environment. Further, it can be improved with Double DQN \cite{Qrash:_DQN} and by providing more environmental details. 



