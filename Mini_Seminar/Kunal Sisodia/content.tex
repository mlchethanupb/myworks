%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START ADDING TEXT HERE 
%
% Feel free to use \include commands to structure text in smaller
% pieces 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Abstract gives a brief summary of the main points of a paper:
\begin{abstract}
  This paper gives a brief overview of an experience-driven approach in communication networking to counter a predicament of traffic engineering. The approach is self-sufficient to manage a fully modern communication network with a higher level of complexities and dynamicity. This approach outcasts all the other currently used methods in terms of efficiency.
\end{abstract}

% the actual content, usually separated over a number of sections
% each section is assigned a label, in order to be able to put a
% crossreference to it

\section{Introduction}
\label{sec:introduction}

\begin{itemize}

\item Enhancing the efficiency of modern communication network.
\item Development of DRL-TE Approach. 
\item Benefits of DRL-TE Approach.
\item Drawbacks of DRL-TE Approach.
 
\end{itemize}


% Note: for a SEMINAR, Related Work usually is a BAD idea and makes no
% sense! 

\section{Problem Statement}
\label{sec:problem}
\begin{itemize}

\item Describing Traffic Engineering (TE) Problem. 
\item Objective to maximize the utility function.
 
\end{itemize}

\section{Network Utility Maximization (NUM)}
\begin{itemize}
\item A solution to an optimization problem \cite{low_Lapsley:_handb_Flowcontrol}

\item Drawbacks associated with NUM. 

\item Algorithm combined with SDN (Software Defined Networking). 

\label{sec:NUM}
\end{itemize}

\section{DEEP REINFORCEMENT LEARNING (DRL)}
\label{sec:relwork}

\begin{itemize}

\item Brief Overview about DRL\cite{v_mnih:_handb_DRL}.

\item Key concepts of Deep Q-Network.

\item Actor-critic method - Deep Deterministic Policy Gradient\cite{leuwen00:_handb_schol}

\end{itemize}

\section{Proposed DRL Based Framework}
\label{sec:Proposed_DRL}
\begin{itemize}

\item Design of State space, Action space and reward.
\item Extending DDPG approach.
\item Key points like Temporal Difference and Q gradients were defined.
\item Prioritized experience replay\cite{Schaul:_replay}
\item The working of DRL-TE algorithm is discussed in brief.
\end{itemize}

\section{Algorithm DRL-TE}
\label{sec:orgheadline1}
\begin{itemize}

\item 2-layer fully connected feed forward neural network configuration for actor and critical network
\item Inclusion of Leaky Rectifier.  
\item Utilization of Softmax.
\end{itemize}

\subsection{Implementation of Algorithm DRL-TE}
\label{sec:orgheadline1}
\begin{itemize}

\item Initialization of all weights of actor and critic network.
\item Computation of various key elements - gradients, priority, weight change etc.
\item Updating Network Parameters.
\item Implementation of the algorithm using ns-3 for packet level simulation
\end{itemize}



\input{petpeeves}

\section{Discussion}
\label{sec:orgheadline1}

(with references 1/2 page)
\begin{itemize}
\item An experience driven approach.
\item DRL-TE compared with NUM and DDPG
\end{itemize}

\section{Conclusion}
\label{sec:concl}

In the end , authors provided an experience-driven DRL-TE with actor and critic networks approach to counter TE problem.
The performance of this approach when evaluated using NSFNET, APRANET topologies and a random topologies outperforms all the current baselines methods.