\begin{abstract}
Abstract. Virtualizing network functions (e.g. firewall or router) is nowadays one concept to build a network. Instead of producing specialized hardware, they can be deployed on common servers which reduces in particular the arising cost for changing properties. For example is the scaling and updating of network functions much easier and faster. However, this comes with a lot of new problems. One is the orchestration and the scheduling of traffic flows of several connected network functions on different servers. 
For a service provider a good and efficient solution is necessary to get a good revenue with less cost for his services.\\
This NP-hard problem can be solved exactly, heuristically or with the use of machine learning, in particular deep reinforcement learning (DRL). Exact solutions and the training of DRL algorithms are slow, because of the complexity of the problem and heuristic algorithms are mostly developed for specific cases, containing some assumptions. However, a DRL algorithm can be speed up by not just searching random in the action space, but by guiding the DRL agent with a good heuristic. This guidance can improve the convergence speed of the training by a factor of 23 while also improving the performance of the DRL algorithm~\cite {Gu}.
\end{abstract}

\section{Introduction}%with abstract 1 page
\label{sec:introduction}
Consider a service provider want to offer some service (e.g. internet) that consists of multiple functions (e.g. firewall, router, DPI\dots). The service provider could just produce physical systems, one for every function, but this is neither flexibility or scalability nor saves money. So a virtualization of the network functions is necessary to run them on common servers that can be rented from infrastructure providers. But now a few and also new problems arise that must be answered: What amount of a specific network function is needed? On which of the multiple possible servers from the infrastructure provider should the network function be activated? If it was decided to deploy more then one of a specific network function, to which of them should an incoming flow be scheduled?\\
This are typical problems for a service provider and it is called the virtual network function (VNF) orchestration and flow scheduling problem. Orchestration is the activation and deactivation of a VNF at a server and flow scheduling deals with the question to which of the activated VNF the traffic will be directed.  \\
Luckily there are already solutions for this or strongly related problems. This NP-hard problem was solved mostly with Heuristics
or recently with deep reinforcement learning (DRL) (e.g \cite{Guz}). However, heuristic solutions mostly simplify the model, take assumptions (e.g. they do not consider the end-to-end delay) or just solving the problem offline and recent DRL algorithms need a long time until the DRL agent is trained well if the action or the state space is large. Therefore Gu et al. \cite{Gu} proposed a framework for DRL that let the training converge 23 times faster due to the guidance of the agent with a heuristic compared to the standard DRL approach.



\section{Model} %1 page
\label{sec:model}

\begin{figure*}[t]
\label{fig:graph}
\centering
\includegraphics[scale=0.6]{graph1}
\caption{This example represents an activation of VNFs at servers from an infrastructure provider at one timestep. The numbers represent the different network functions (namely 1,2,3,4 and 5).\\ On the top left are two SFCs that have to be installed on the servers from the infrastructure provider.\\ At the top right is the infrastructure graph with the servers, the deployed VNFs and the links between them from the provider. \\ On the button is one possible activation for the two SFCs. Note that a VNF can be shared by multiple SFC and there can be multiple of one network function deployed in the network. If the flow rate is low, you could also choose to activate just one of VNF 2 or to activate two VNFs at one server from the infrastructure provider. If the flow rate is high you could also activate more VNFs (except for the start/origin function which must be unambiguous)}
\end{figure*}

This explanation follows the formal model in \cite{Gu}.\\
In the VNF orchestration and flow scheduling problem servers from a infrastructure provider are given as an indirected graph. A server is represented as a node and a connection of two servers as a link in the graph.\\
 Also set of service function chains (SFC) are given as a directed graph, where a node represents a network function. A SFC can be seen as a service in which different network functions must be executed in a predefined order. If a network function must be executed before an other network function it will be represented as an directed edge in the graph. The goal is to activate deployed VNFs on the servers of the infrastructure provider with low cost. An example can be seen in Figure 1.\\%\ref{fig:graph}. \ref seems not to work...


The costs that are arising using a network from a infrastructure provider are put together as follows:
\begin{enumerate}
\item The setup cost (e.g. booting the virtual machine). If you let a VNF activated over multiple timesteps this cost just arises one time.
\item The cost for using a server, depending on e.g. the operation time and the cost per hour from the infrastructure provider for the specific server.

\item Communication cost for sending data between different servers. This cost depend on the amount of data sent.
\item End to end delay. This cost grows logarithmic with the delay of a flow, because one second delay is worse if you do not have a delay than if you already have 2 minutes of delay.
\end{enumerate}
The revenue is the money you get for the incoming flows. Hence, the utility is the revenue (the payment) minus the costs, which has to be maximized. However, if the flow rate can not be predicted the utility can not be calculated beforehand. In this case the instant-utility which has to be minimized is defined as the summation of all costs without revenue and end to end delay.


%\section{Model-Assisted DRL Framework}%1 page
%\label{sec:drl}
\begin{figure*}[t]
\label{fig:framework}
\centering
\includegraphics[scale=0.45]{framework}
\caption{The DRL framework. It explains the different steps of the framework to optimize the DRL algorithm. Source: \cite{Gu}, Figure~1.}
\end{figure*}
\section{Model-Assisted DRL Framework}%1 page
\label{sec:drl}
The core idea of \cite{Gu} is that they use the deep deterministic policy gradient (DDPG) with the guidance of a heuristic. The DDPG algorithm needs as input tuples, containing the current state, an action, the reward and the next state, but instead of letting the agent explore the action space randomly at the beginning, they use a heuristic to create helpful tuples for the training.\\
 In \cite{Gu} the state is an vector containing the currently activated VNF at all servers and the flow rate between them. The action will be the modification of the flow rate and the activation status and the reward is the utility.\\
The model-assisted DRL Framework can now be summarized into the six steps in Figure 2:%\ref{fig:framework}:
\begin{enumerate}
\item At first the current state (activated VNFs and the current flow rate) must be observed from the environment.
\item Samples must be created to train the DRL-agent. A sample must contain the current state, an action, the reward and the next state you will get. They propose two algorithms for picking the action based on the current observed state: \\
Fist a bias-exploration agent where the agent explores the action space by himself which is just a slightly changed version of the normal exploration in DDPG, where the probability for edge cases is higher (e.g. flow rate between VNFs is 0). \\
Second a profiling action generation algorithm which is creating the action based on a heuristic (based on the utility, if the flow rate can be predicted, or instant-utility if not). \\
It is possible to only pick one algorithm. However, a better way according to the simulations is to use at the beginning the profiling action generation algorithm because the agent can learn better from the good actions of the heuristic than from his inexperienced choices. When the agent has got experience the exploration is used more so that the agent can also explore by himself. Because the profiling action generation algorithm can just generate actions based on the optimization in one timeslot, the exploration is necessary for learning  particularly optimizations over multiple timesteps.
\item  The reward for the action and the next state will be calculated.
\item Step 1-3 are repeated for training until enough samples have been created. A sample is saved in the exploration and profiling replay buffer respectively.
\item The agent gets trained with a batch of samples from the buffer.
\item The agent gets updated.
\end{enumerate}

\section {Implementation}
The pseudocode for the framework is given in the paper, the rest (e.g. environment, solving ILP/LP problems \dots) must be implemented without help of pseudocode \cite{Gu}. There is no public implementation available.


\section{Evaluation}% 1/2 page
\label{sec:evaluation}
Gu et al. \cite{Gu} evaluate their approach with other DRL algorithms in particular also with their previous work \cite{Guz}. The exact setup is also mentioned in the paper \cite{Gu}. Roughly they used a real network topology with 43 nodes, embed 10 SFCs in it and use the instant-utility based heuristic for their algorithm. The SFC uses 30 different VNFs.\\
Their evaluation showed that their algorithm is faster then all of the other DRL algorithms compared with (23 times faster against the standard DRL approach and 19 times faster compared to their previous work). Though they need more time for one training step because of the calculation of the heuristic, the algorithm needs mush less episodes to converge to a good reward.\\
Furthermore they tested their algorithm with different variables (i.e. when to switch from the profiling action generation algorithm to the exploration algorithm). A too short or a too long use of the profiling action generation algorithm leads to a bad result. Already mentioned in chapter 3 %%
the reason is that a short use of the algorithm leads to random actions in the beginning and a long use to overfitting because the instant-utility just optimizes for one timeslot and do not consider the actual end to end delay. With a good balance of the algorithms their approach leads to better results even after training. This is also the reason why the DRL algorithm with the heuristic based on the instant-utility performs a bit worse than the DRL algorithm with the utility based heuristic, but the difference between them is not as huge as you would expect. Just using a heuristic solution (that do not consider end to end delay) without DRL achieves in their setup in the most cases the worst performance.




\section{Discussion} %with references 1/2 page, maybe less
\label{sec:relwork}
There are mainly 2 interesting questions to discuss.\\
First: How does this work compares against other DRL algorithms in particular against their previous work \cite{Guz}. It is quiet obvious that a DRL algorithm without guidance performs worse than the same DRL algorithm with guidance. However, their DRL algorithm also works better than their previous work. The main difference is that the predecessor for the algorithm just uses the bias-exploration method mentioned in chapter 3. %%
So instead of having a profiling replay buffer they used a baseline buffer. Instead of putting tuples created from the instant-utility based algorithm into that buffer, they putting tuples from the bias-exploration that are better than the solution of instant-utility in it. This slight change together without the possibility to change the approach while the agent gets more experience leads to a worse result.  \\

Second: Can you use the idea of the framework to improve other DRL algorithms even in other fields.
The simple answer is yes you can. This framework could be build on top of other DRL-algorithms and also works with other problems. For other environments you need to deploy a heuristic for that specific problem and for other DRL algorithms you obviously need to implement the DRL algorithm, but how this framework behaves in this cases has not been researched yet and will be the most interesting question for the future.




 