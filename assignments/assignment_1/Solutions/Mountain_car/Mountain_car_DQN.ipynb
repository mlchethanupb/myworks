{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Members**\n",
    "* Aluri JaganMohini\n",
    "* Chethan Lokesh Mariyaklla\n",
    "* Priyanka Giri \n",
    "* Tejas Ravindra Dhawale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mountain car is an environment where a car must climb a mountain. Because gravity is stronger than the car's engine, even with full throttle, it cannot merely accelerate up the steep slope. The vehicle is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the car can make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "![title](img/mountaincar.jpg)\n",
    "\n",
    "**Observation** \n",
    "* state[0] - Position \n",
    "    | min: -1.2, max: 0.6\n",
    "* state[1] - Velocity \n",
    "     | min: -0.7, max: 0.7\n",
    "\n",
    "\n",
    "**Actions**:\n",
    "* 0 - Push left\n",
    "* 1 - No push\n",
    "* 2 - push right\n",
    "\n",
    "\n",
    "**Reward**\n",
    "* -1 for each time step, until the goal position of 0.5 is reached. \n",
    "\n",
    "\n",
    "**Starting State**\n",
    "* Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "\n",
    "**Episode Termination**\n",
    "* The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "**Code reference from:** *https://github.com/coolsgupta/deep_q_learning/blob/master/Code/Mountain_car/DQN_model_1.py*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import csv\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.model1.hdf5',\n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0, callbacks=[agent.checkpointer])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == 0:\n",
    "            if (state[0][1] > 0):\n",
    "                return -1\n",
    "\n",
    "            elif (state[0][1] == 0):\n",
    "                if (next_state[0][0] < state[0][0]):\n",
    "                    return 1\n",
    "                else :\n",
    "                    return -1\n",
    "            elif (state[0][1] < 0):\n",
    "                return 1\n",
    "\n",
    "        elif action == 1:\n",
    "            if (state[0][1] == 0):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        elif action == 2:\n",
    "            if (state[0][1] > 0):\n",
    "                return 1\n",
    "\n",
    "            elif (state[0][1] == 0):\n",
    "                if (next_state[0][0] > state[0][0]):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            elif (state[0][1] < 0):\n",
    "                return -1\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "# initialize gym environment and the agent\n",
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "done = False\n",
    "batch_size = 32\n",
    "# Iterate the game\n",
    "#storing results\n",
    "result_csv = open('results_DQN_model_1.csv','w',newline='')\n",
    "fieldnames = ['episode', 'epsilon', 'score', 'average_score', 'total_reward', 'average_reward']\n",
    "result_writer = csv.DictWriter(result_csv, fieldnames)\n",
    "result_writer.writeheader()\n",
    "cummulative_score = 0\n",
    "cummulative_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    total_reward = 0\n",
    "    score = state[0][0]\n",
    "    # time_t represents each frame of the game\n",
    "    for time in range(200):\n",
    "        #for GUI\n",
    "        #env.render()\n",
    "\n",
    "        #Decide action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        total_reward += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        score = next_state[0][0] if next_state[0][0]>score else score\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        created_reward = agent.get_reward(state, action, next_state)\n",
    "        agent.remember(state, action, created_reward, next_state, done)\n",
    "\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print and store the metrics and break out of the loop\n",
    "            cummulative_reward += total_reward\n",
    "            average_reward = cummulative_reward/(e+1)\n",
    "            cummulative_score += score\n",
    "            average_score = cummulative_score/(e+1)\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, score, agent.epsilon))\n",
    "            result_writer.writerow({'episode':e, 'epsilon':agent.epsilon, 'score':score, 'average_score':average_score,\n",
    "                                    'total_reward':total_reward, 'average_reward':average_reward})\n",
    "            break\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "# testing trials\n",
    "print(\"///////////TESTING/////////\")\n",
    "agent.epsilon = 0.0\n",
    "cummulative_score = 0\n",
    "cummulative_reward = 0\n",
    "test_results = open('test_results_DQN_model_1.csv', 'w',newline='')\n",
    "result_writer = csv.DictWriter(test_results,fieldnames)\n",
    "result_writer.writeheader()\n",
    "for e in range(50):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    score = state[0][0]\n",
    "    # time_t represents each frame of the game\n",
    "    for time in range(200):\n",
    "        # for GUI\n",
    "        env.render()\n",
    "\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        total_reward += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        score = next_state[0][0] if next_state[0][0] > score else score\n",
    "\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print and store the metrics and break out of the loop\n",
    "            cummulative_reward += total_reward\n",
    "            average_reward = cummulative_reward / (e+1)\n",
    "            cummulative_score += score\n",
    "            average_score = cummulative_score / (e+1)\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, 50, score, agent.epsilon))\n",
    "            result_writer.writerow({'episode':e, 'epsilon':agent.epsilon, 'score':score, 'average_score':average_score,\n",
    "                                    'total_reward': total_reward, 'average_reward':average_reward})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
