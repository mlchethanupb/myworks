{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from copy import deepcopy\n",
    "from ray import tune\n",
    "from stable_baselines.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import w_mac\n",
    "from collections import defaultdict\n",
    "import matplotlib as plt\n",
    "import networkx as nx\n",
    "\n",
    "# data = [(0,2),(0,1),(1,2),(2,3),(2,4),(3,4)]\n",
    "#data = [(0,2),(0,1),(0,3),(1,2),(1,3),(2,3),(2,4),(3,4),(5,2),(5,3),(5,4),(5,6),(6,7),(6,8),(7,8),(8,9),(9,10),(4,10)]#(4,6),(5,10),(6,10),(9,6),(8,10)]\n",
    "d = defaultdict(list)\n",
    "data = [(0,2),(0,1),(0,3),(1,2),(1,3),(2,3),(2,4),(3,4),(5,2),(5,3),(5,4)]\n",
    "# defaultdict(<type 'list'>, {})\n",
    "for node, dest in data:\n",
    "    d[node].append(dest)\n",
    "print(d)\n",
    "\n",
    "G = nx.Graph()\n",
    "for k,v in d.items():\n",
    "    for vv in v:\n",
    "        G.add_edge(k,vv)\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('wmac-graph-v0',graph=G)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        self.is_tb_set = False\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log additional tensor\n",
    "        if not self.is_tb_set:\n",
    "            with self.model.graph.as_default():\n",
    "                tf.summary.scalar('packet_lost', tf.reduce_mean(env.get_packet_lost()))\n",
    "                self.model.summary = tf.compat.v1.summary.merge_all()\n",
    "            self.is_tb_set = True\n",
    "        # Log scalar value (here a random variable)\n",
    "        value = env.get_packet_lost()\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag='packet_lost', simple_value=value)])\n",
    "        self.locals['writer'].add_summary(summary, self.num_timesteps)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = A2C(MlpPolicy, env, verbose=1,tensorboard_log=\"./a2c_tensorboard/\")\n",
    "\n",
    "model.learn(total_timesteps=1000000, callback=TensorboardCallback())\n",
    "model.save(\"a2c_wmac_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = A2C(MlpPolicy, \n",
    "            env, \n",
    "            verbose=1,\n",
    "            gamma = 0.9980934451241046,\n",
    "            max_grad_norm = 4.3428906664252285,\n",
    "            learning_rate= 0.0018115682523418219,\n",
    "            tensorboard_log=\"./a2c_tensorboard_tuned/\")\n",
    "\n",
    "model.learn(total_timesteps=500000,\n",
    "            callback=TensorboardCallback())\n",
    "model.save(\"a2c_wmac_small_tuned\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = A2C.load(\"a2c_wmac_small_tuned\")\n",
    "\n",
    "obs = env.reset()\n",
    "count = 0\n",
    "while count < 5000:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    count = count + 1\n",
    "    print(\"count :\", count)\n",
    "    if done:\n",
    "        # clear_output(wait = True)\n",
    "        #time.sleep(3)\n",
    "        env.render()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_objective(config):\n",
    "#     tune_env = deepcopy(env)\n",
    "#     tune_agent = A2C(\"MlpPolicy\", tune_env, learning_rate= config[\"lr\"] )#**config)\n",
    "#     tune_agent.learn(total_timesteps=10)\n",
    "    \n",
    "#     mean_reward, std_reward = evaluate_policy(tune_agent, env, n_eval_episodes=10, render=False,\n",
    "#                                                            deterministic=True,\n",
    "#                                                            return_episode_rewards=False)\n",
    "#     print(\"mr\",mean_reward,\"sd\", std_reward)\n",
    "#     tune.report(mean_reward=mean_reward)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray import tune\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     evaluate_objective,\n",
    "#     #num_samples=3,\n",
    "#     config={\n",
    "#         \"lr\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "#     },\n",
    "#     metric=\"mean_reward\",\n",
    "#     mode=\"max\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_config = analysis.best_config\n",
    "# print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
