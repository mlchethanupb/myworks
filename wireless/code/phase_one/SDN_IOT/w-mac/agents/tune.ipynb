{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "import tensorflow as tf\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.bench.monitor import Monitor\n",
    "from stable_baselines.common.callbacks import EveryNTimesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import w_mac\n",
    "from collections import defaultdict\n",
    "import matplotlib as plt\n",
    "import networkx as nx\n",
    "import dill\n",
    "from copy import deepcopy\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = [(0,2),(0,1),(1,2),(2,3),(2,4),(3,4)]\n",
    "data = [(0,2),(0,1),(0,3),(1,2),(1,3),(2,3),(2,4),(3,4),(5,2),(5,3),(5,4),(5,6),(6,7),(6,8),(7,8),(8,9),(9,10),(4,10)]#(4,6),(5,10),(6,10),(9,6),(8,10)]\n",
    "d = defaultdict(list)\n",
    "#data = [(0,2),(0,1),(0,3),(1,2),(1,3),(2,3),(2,4),(3,4),(5,2),(5,3),(5,4)]\n",
    "for node, dest in data:\n",
    "    d[node].append(dest)\n",
    "print(d)\n",
    "\n",
    "G = nx.Graph()\n",
    "for k,v in d.items():\n",
    "    for vv in v:\n",
    "        G.add_edge(k,vv)\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('wmac-graph-v0',graph=G)\n",
    "#env = gym.make('wmac-tune-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(dill.pickles(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def evaluate_objective(config):\n",
    "    tune_env = deepcopy(env)\n",
    "    tune_agent = A2C(\"MlpPolicy\", tune_env, learning_rate= config[\"lr\"] )\n",
    "    tune_agent.learn(total_timesteps=10)\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(tune_agent, env, n_eval_episodes=10, render=False,\n",
    "                                                           deterministic=True,\n",
    "                                                           return_episode_rewards=False)\n",
    "    print(\"mr\",mean_reward,\"sd\", std_reward)\n",
    "    tune.report(mean_reward=mean_reward)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "from ray.tune import report\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecEnv, sync_envs_normalization\n",
    "\n",
    "class OptimizationCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, eval_env: Union[gym.Env, VecEnv],\n",
    "                 n_eval_episodes: int = 5,\n",
    "                 deterministic: bool = True,\n",
    "                 verbose=0):\n",
    "        super(OptimizationCallback, self).__init__(verbose)\n",
    "        self.eval_env = deepcopy(eval_env)\n",
    "        self.eval_env.reset()\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def _on_step(self):\n",
    "        #sync_envs_normalization(self.training_env, self.eval_env)\n",
    "\n",
    "        episode_rewards, episode_lengths = evaluate_policy(self.model, self.eval_env,\n",
    "                                                           n_eval_episodes=self.n_eval_episodes,\n",
    "                                                           render=False,\n",
    "                                                           deterministic=self.deterministic,\n",
    "                                                           return_episode_rewards=True)\n",
    "\n",
    "        mean_reward= np.mean(episode_rewards)\n",
    "        mean_ep_length = np.mean(episode_lengths)\n",
    "        packet_lost = self.eval_env.get_packet_lost()\n",
    "\n",
    "        report(\n",
    "            mean_reward=mean_reward,\n",
    "            mean_ep_length=mean_ep_length,\n",
    "            packet_lost = packet_lost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import ray\n",
    "\n",
    "object_store_memory = int(0.3 * ray.utils.get_system_memory() // 10 ** 9 * 10 ** 9)\n",
    "ray.init(\n",
    "            ignore_reinit_error=True,\n",
    "            num_cpus = 10,\n",
    "            local_mode = True,\n",
    "            _plasma_directory=\"/tmp\",\n",
    "            object_store_memory=object_store_memory,\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "parameters=[\n",
    "    {\"name\": \"learning_rate\", \"type\": \"range\", \"bounds\": [3e-5, 3e-3]},\n",
    "    {\"name\": \"gamma\", \"type\": \"range\", \"bounds\": [0.99, 1.0]},\n",
    "    {\"name\": \"max_grad_norm\", \"type\": \"range\", \"bounds\": [0.3, 7.0]}\n",
    "]   \n",
    "\n",
    "        \n",
    "def evaluate_objective(config):\n",
    "    tune_env = deepcopy(env)\n",
    "    tune_monitor = OptimizationCallback(tune_env, 5, True)\n",
    "    monitor_callback = EveryNTimesteps(n_steps=10000, callback=tune_monitor)\n",
    "\n",
    "    \n",
    "    \n",
    "    tune_agent = A2C(\"MlpPolicy\", tune_env, \n",
    "                     gamma = config[\"gamma\"],\n",
    "                     max_grad_norm = config[\"max_grad_norm\"],\n",
    "                     learning_rate= config[\"learning_rate\"]\n",
    "                    )#**config)\n",
    "    tune_agent.learn(total_timesteps=1000000, callback=monitor_callback)\n",
    "    \n",
    "ax_search = AxSearch(space=parameters, metric=\"mean_reward\", mode = \"max\")\n",
    "\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='mean_reward',\n",
    "    mode='max',\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)\n",
    "\n",
    "analysis = tune.run(evaluate_objective,\n",
    "         num_samples=5,\n",
    "         search_alg=ax_search,\n",
    "         scheduler=asha_scheduler,\n",
    "         resources_per_trial={\"cpu\": 8}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis.get_best_config(metric=\"mean_reward\", mode = \"max\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
