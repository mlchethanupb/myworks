# Minutes of meeting from 2020-08-21


## Points Discussed
- Discussed changes made to the old code
    - Converting env to multi hop env.
    - Penalty to agent when all the actions are 0 when isdone = false and action taken on empty queue.
    - PPO2 dint work as expected will stick with the A2C.
    - Modified code for monitoring the waiting time of each node by adding the counter -- but agent was not trained proper manner
    - The current behaviour of environment - TDMA
    - Normalize the reward points to 1 or -1
    

## Administrative
- One to one meeting on October 2, 2020

## Action Points
- When packet lengthis 0 do not increase the counter value (waiting time).
- Waiting time and number of nodes in graph to be made dynamically.
- Visualize the results -- tensorboard graph.
- Correct reward for the counter.
- Entire team meeting
    - Explain problem, how the agent is learning?
    - Visualize the results -- add figures
- With respect to SDN environment
    - Consider Queue length as flow table
    - Add more nodes to the intermediate nodes -- under attacked node situation

